{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e31d49",
   "metadata": {},
   "source": [
    "# Food Price Data Source\n",
    "\n",
    "[WFP Food Prices Kenya Dataset](https://data.humdata.org/dataset/wfp-food-prices-for-kenya)\n",
    "\n",
    "# Rainfall Data Source\n",
    "[WFP Rainfall Kenya Dataset](https://data.humdata.org/dataset/ken-rainfall-subnational)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28254e24",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880df77",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be44816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a0500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/15 05:22:53 WARN Utils: Your hostname, codespaces-ebd91c resolves to a loopback address: 127.0.0.1; using 10.0.1.23 instead (on interface eth0)\n",
      "25/04/15 05:22:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/15 05:22:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FoodData'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 46304)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('FoodData').master('local[*]').getOrCreate()\n",
    "\n",
    "spark.sparkContext.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31e0776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- commodity: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- pricetype: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"data/wfp_food_prices_ken_data.csv\",inferSchema=True,header=True)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936b52ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+--------+------------------+-------------+-----+---------+------+\n",
      "|     date| region|  county|  market|          category|    commodity| unit|pricetype| price|\n",
      "+---------+-------+--------+--------+------------------+-------------+-----+---------+------+\n",
      "|1/15/2006|  Coast| Mombasa| Mombasa|cereals and tubers|        Maize|   KG|Wholesale| 16.13|\n",
      "|1/15/2006|Eastern|   Kitui|   Kitui|cereals and tubers|      Sorghum|90 KG|Wholesale|1800.0|\n",
      "|1/15/2006|Eastern|   Kitui|   Kitui|   pulses and nuts|  Beans (dry)|   KG|   Retail|  39.0|\n",
      "|1/15/2006|Eastern|Marsabit|Marsabit|cereals and tubers|Maize (white)|   KG|   Retail|  21.0|\n",
      "|1/15/2006|Nairobi| Nairobi| Nairobi|cereals and tubers|        Bread|400 G|   Retail|  26.0|\n",
      "+---------+-------+--------+--------+------------------+-------------+-----+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d26fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- rainfall_mm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rainfall = spark.read.csv(\"data/ken-rainfall-data.csv\",inferSchema=True,header=True)\n",
    "rainfall.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33ecb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     date|rainfall_mm|\n",
      "+---------+-----------+\n",
      "| 1/1/1981|       NULL|\n",
      "|1/11/1981|       NULL|\n",
      "|1/21/1981|       NULL|\n",
      "| 2/1/1981|       NULL|\n",
      "|2/11/1981|       NULL|\n",
      "+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rainfall.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae41328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert date strings to DateType\n",
    "data = data.withColumn(\"date\", to_date(\"date\", \"M/d/yyyy\"))\n",
    "rainfall = rainfall.withColumn(\"date\", to_date(\"date\", \"M/d/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6958385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+------------------+-------------+-----+---------+------+-----+----+\n",
      "| region|  county|  market|          category|    commodity| unit|pricetype| price|month|year|\n",
      "+-------+--------+--------+------------------+-------------+-----+---------+------+-----+----+\n",
      "|  Coast| Mombasa| Mombasa|cereals and tubers|        Maize|   KG|Wholesale| 16.13|    1|2006|\n",
      "|Eastern|   Kitui|   Kitui|cereals and tubers|      Sorghum|90 KG|Wholesale|1800.0|    1|2006|\n",
      "|Eastern|   Kitui|   Kitui|   pulses and nuts|  Beans (dry)|   KG|   Retail|  39.0|    1|2006|\n",
      "|Eastern|Marsabit|Marsabit|cereals and tubers|Maize (white)|   KG|   Retail|  21.0|    1|2006|\n",
      "|Nairobi| Nairobi| Nairobi|cereals and tubers|        Bread|400 G|   Retail|  26.0|    1|2006|\n",
      "+-------+--------+--------+------------------+-------------+-----+---------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Split the 'date' column into 'month' and 'year'\n",
    "data1 = data.withColumn('month', F.month('date')) \\\n",
    "                       .withColumn('year', F.year('date'))\n",
    "data1 = data1.drop('date')\n",
    "\n",
    "data1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d76148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+--------+---------+----+---------+-----+\n",
      "|date|region|county|market|category|commodity|unit|pricetype|price|\n",
      "+----+------+------+------+--------+---------+----+---------+-----+\n",
      "|   0|    44|    44|     0|       0|        0|   0|        0|    0|\n",
      "+----+------+------+------+--------+---------+----+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|date|rainfall_mm|\n",
      "+----+-----------+\n",
      "|   0|        584|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count nulls in each column for 'data' DataFrame\n",
    "null_counts = data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "null_counts.show()\n",
    "\n",
    "# Count nulls in each column for 'rainfall' DataFrame\n",
    "rainfall_null_counts = rainfall.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in rainfall.columns])\n",
    "rainfall_null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cba691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data.dropna()\n",
    "rainfall_clean = rainfall.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05cb170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12702, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.count(), len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c3896a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115705, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainfall_clean.count(), len(rainfall.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b561149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+--------+------------------+-------------+-----+---------+------+\n",
      "|      date| region|  county|  market|          category|    commodity| unit|pricetype| price|\n",
      "+----------+-------+--------+--------+------------------+-------------+-----+---------+------+\n",
      "|2006-01-15|  Coast| Mombasa| Mombasa|cereals and tubers|        Maize|   KG|Wholesale| 16.13|\n",
      "|2006-01-15|Eastern|   Kitui|   Kitui|cereals and tubers|      Sorghum|90 KG|Wholesale|1800.0|\n",
      "|2006-01-15|Eastern|   Kitui|   Kitui|   pulses and nuts|  Beans (dry)|   KG|   Retail|  39.0|\n",
      "|2006-01-15|Eastern|Marsabit|Marsabit|cereals and tubers|Maize (white)|   KG|   Retail|  21.0|\n",
      "|2006-01-15|Nairobi| Nairobi| Nairobi|cereals and tubers|        Bread|400 G|   Retail|  26.0|\n",
      "+----------+-------+--------+--------+------------------+-------------+-----+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fca5c6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      date|rainfall_mm|\n",
      "+----------+-----------+\n",
      "|1981-03-21|   266.3542|\n",
      "|1981-04-01|     360.75|\n",
      "|1981-04-11|      542.5|\n",
      "|1981-04-21|   608.1042|\n",
      "|1981-05-01|   767.2083|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rainfall_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee42c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+------------------+-------------+-----+---------+------+-----+----+\n",
      "| region|  county|  market|          category|    commodity| unit|pricetype| price|month|year|\n",
      "+-------+--------+--------+------------------+-------------+-----+---------+------+-----+----+\n",
      "|  Coast| Mombasa| Mombasa|cereals and tubers|        Maize|   KG|Wholesale| 38.44|    1|2014|\n",
      "|  Coast| Mombasa| Mombasa|   pulses and nuts|        Beans|   KG|Wholesale| 79.99|    1|2014|\n",
      "|  Coast| Mombasa| Mombasa|   pulses and nuts|  Beans (dry)|90 KG|Wholesale|5738.0|    1|2014|\n",
      "|Eastern|   Kitui|   Kitui|   pulses and nuts|  Beans (dry)|   KG|   Retail|  74.0|    1|2014|\n",
      "|Eastern|Marsabit|Marsabit|cereals and tubers|Maize (white)|   KG|   Retail| 53.36|    1|2014|\n",
      "+-------+--------+--------+------------------+-------------+-----+---------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Split the 'date' column into 'month' and 'year'\n",
    "data_clean = data_clean.withColumn('month', F.month('date')) \\\n",
    "                       .withColumn('year', F.year('date'))\n",
    "\n",
    "# Drop the 'date' column\n",
    "data_clean = data_clean.drop('date')\n",
    "\n",
    "# Filter years between 2014 and 2024 (inclusive)\n",
    "data_clean = data_clean.filter((data_clean.year >= 2014) & (data_clean.year <= 2024))\n",
    "\n",
    "# Sort by year and month\n",
    "data_clean = data_clean.orderBy(\"year\", \"month\")\n",
    "\n",
    "data_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c4ea912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------------+\n",
      "|year|month|avg_rainfall_mm|\n",
      "+----+-----+---------------+\n",
      "|2014|    1|         259.33|\n",
      "|2014|    2|         201.62|\n",
      "|2014|    3|         184.92|\n",
      "|2014|    4|         269.88|\n",
      "|2014|    5|         339.76|\n",
      "+----+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "# Split the 'date' column into 'month' and 'year'\n",
    "rainfall_clean = rainfall_clean.withColumn('month', F.month('date')) \\\n",
    "                               .withColumn('year', F.year('date'))\n",
    "\n",
    "# Drop the 'date' column\n",
    "rainfall_clean = rainfall_clean.drop('date')\n",
    "\n",
    "# Group by year and month, and calculate average rainfall rounded to 2 decimal places\n",
    "rainfall_clean = rainfall_clean.groupBy(\"year\", \"month\").agg(\n",
    "    round(avg(\"rainfall_mm\"), 2).alias(\"avg_rainfall_mm\")\n",
    ")\n",
    "\n",
    "# Filter years between 2014 and 2024 (inclusive)\n",
    "rainfall_clean = rainfall_clean.filter((rainfall_clean.year >= 2014) & (rainfall_clean.year <= 2024))\n",
    "\n",
    "# Sort by year and month\n",
    "rainfall_clean = rainfall_clean.orderBy(\"year\", \"month\")\n",
    "\n",
    "# Show first 5 rows\n",
    "rainfall_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "329e7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/15 05:23:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+--------+--------+------------------+-------------+-----+---------+------+---------------+\n",
      "|year|month| region|  county|  market|          category|    commodity| unit|pricetype| price|avg_rainfall_mm|\n",
      "+----+-----+-------+--------+--------+------------------+-------------+-----+---------+------+---------------+\n",
      "|2014|    1|  Coast| Mombasa| Mombasa|cereals and tubers|        Maize|   KG|Wholesale| 38.44|         259.33|\n",
      "|2014|    1|  Coast| Mombasa| Mombasa|   pulses and nuts|        Beans|   KG|Wholesale| 79.99|         259.33|\n",
      "|2014|    1|  Coast| Mombasa| Mombasa|   pulses and nuts|  Beans (dry)|90 KG|Wholesale|5738.0|         259.33|\n",
      "|2014|    1|Eastern|   Kitui|   Kitui|   pulses and nuts|  Beans (dry)|   KG|   Retail|  74.0|         259.33|\n",
      "|2014|    1|Eastern|Marsabit|Marsabit|cereals and tubers|Maize (white)|   KG|   Retail| 53.36|         259.33|\n",
      "+----+-----+-------+--------+--------+------------------+-------------+-----+---------+------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join market price data with rainfall data on year and month\n",
    "food_price_data = data_clean.join(\n",
    "    rainfall_clean,\n",
    "    on=[\"year\", \"month\"],  # Join keys\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Show sample of the joined result\n",
    "food_price_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3058b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+------+------+--------+---------+----+---------+-----+---------------+\n",
      "|year|month|region|county|market|category|commodity|unit|pricetype|price|avg_rainfall_mm|\n",
      "+----+-----+------+------+------+--------+---------+----+---------+-----+---------------+\n",
      "|   0|    0|     0|     0|     0|       0|        0|   0|        0|    0|              0|\n",
      "+----+-----+------+------+------+--------+---------+----+---------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count nulls\n",
    "nulls = food_price_data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in food_price_data.columns])\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b4631bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values for column: year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "|2015|\n",
      "|2023|\n",
      "|2022|\n",
      "|2014|\n",
      "|2019|\n",
      "|2020|\n",
      "|2016|\n",
      "|2024|\n",
      "|2017|\n",
      "|2021|\n",
      "+----+\n",
      "\n",
      "\n",
      "Unique values for column: month\n",
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|12   |\n",
      "|1    |\n",
      "|6    |\n",
      "|3    |\n",
      "|5    |\n",
      "|9    |\n",
      "|4    |\n",
      "|8    |\n",
      "|7    |\n",
      "|10   |\n",
      "|11   |\n",
      "|2    |\n",
      "+-----+\n",
      "\n",
      "\n",
      "Unique values for column: region\n",
      "+-------------+\n",
      "|region       |\n",
      "+-------------+\n",
      "|Rift Valley  |\n",
      "|Eastern      |\n",
      "|North Eastern|\n",
      "|Nyanza       |\n",
      "|Coast        |\n",
      "|Central      |\n",
      "|Nairobi      |\n",
      "+-------------+\n",
      "\n",
      "\n",
      "Unique values for column: county\n",
      "+-----------+\n",
      "|county     |\n",
      "+-----------+\n",
      "|Uasin Gishu|\n",
      "|Nakuru     |\n",
      "|Mandera    |\n",
      "|Kisumu     |\n",
      "|Marsabit   |\n",
      "|Wajir      |\n",
      "|Kajiado    |\n",
      "|Turkana    |\n",
      "|Mombasa    |\n",
      "|Kwale      |\n",
      "|Makueni    |\n",
      "|Meru South |\n",
      "|Garissa    |\n",
      "|Nairobi    |\n",
      "|Isiolo     |\n",
      "|Kitui      |\n",
      "|Kilifi     |\n",
      "|Baringo    |\n",
      "|West Pokot |\n",
      "|Nyeri      |\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Unique values for column: market\n",
      "+-------------------------------+\n",
      "|market                         |\n",
      "+-------------------------------+\n",
      "|Kaanwa (Tharaka Nithi)         |\n",
      "|Illbissil Food Market (Kajiado)|\n",
      "|IFO (Daadab)                   |\n",
      "|Kibra (Nairobi)                |\n",
      "|Dadaab town                    |\n",
      "|Kalahari (Mombasa)             |\n",
      "|Mathare (Nairobi)              |\n",
      "|Kitengela (Kajiado)            |\n",
      "|Kakuma 3                       |\n",
      "|Mukuru (Nairobi)               |\n",
      "|Shonda (Mombasa)               |\n",
      "|Nakuru                         |\n",
      "|Lodwar town                    |\n",
      "|Moroto (Mombasa)               |\n",
      "|Kalobeyei (Village 3)          |\n",
      "|Wote town (Makueni)            |\n",
      "|Dandora (Nairobi)              |\n",
      "|Kakuma 2                       |\n",
      "|Kakuma 4                       |\n",
      "|Wakulima (Nakuru)              |\n",
      "+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Unique values for column: category\n",
      "+---------------------+\n",
      "|category             |\n",
      "+---------------------+\n",
      "|milk and dairy       |\n",
      "|pulses and nuts      |\n",
      "|non-food             |\n",
      "|meat, fish and eggs  |\n",
      "|vegetables and fruits|\n",
      "|oil and fats         |\n",
      "|cereals and tubers   |\n",
      "|miscellaneous food   |\n",
      "+---------------------+\n",
      "\n",
      "\n",
      "Unique values for column: commodity\n",
      "+-------------------------+\n",
      "|commodity                |\n",
      "+-------------------------+\n",
      "|Maize flour              |\n",
      "|Meat (camel)             |\n",
      "|Beans (mung)             |\n",
      "|Milk (cow, pasteurized)  |\n",
      "|Salt                     |\n",
      "|Maize (white)            |\n",
      "|Wheat flour              |\n",
      "|Cowpea leaves            |\n",
      "|Sorghum                  |\n",
      "|Beans (kidney)           |\n",
      "|Beans                    |\n",
      "|Rice (imported, Pakistan)|\n",
      "|Milk (camel, fresh)      |\n",
      "|Maize                    |\n",
      "|Potatoes (Irish, white)  |\n",
      "|Tomatoes                 |\n",
      "|Oil (vegetable)          |\n",
      "|Meat (beef)              |\n",
      "|Cabbage                  |\n",
      "|Beans (dolichos)         |\n",
      "+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Unique values for column: unit\n",
      "+------+\n",
      "|unit  |\n",
      "+------+\n",
      "|Bunch |\n",
      "|400 G |\n",
      "|64 KG |\n",
      "|Head  |\n",
      "|L     |\n",
      "|200 G |\n",
      "|50 KG |\n",
      "|13 KG |\n",
      "|90 KG |\n",
      "|200 ML|\n",
      "|126 KG|\n",
      "|KG    |\n",
      "|Unit  |\n",
      "|500 ML|\n",
      "+------+\n",
      "\n",
      "\n",
      "Unique values for column: pricetype\n",
      "+---------+\n",
      "|pricetype|\n",
      "+---------+\n",
      "|Wholesale|\n",
      "|Retail   |\n",
      "+---------+\n",
      "\n",
      "\n",
      "Unique values for column: price\n",
      "+-------+\n",
      "|price  |\n",
      "+-------+\n",
      "|78.75  |\n",
      "|26.72  |\n",
      "|4800.0 |\n",
      "|53.08  |\n",
      "|67.21  |\n",
      "|6277.33|\n",
      "|38.61  |\n",
      "|85.86  |\n",
      "|82.51  |\n",
      "|2668.8 |\n",
      "|77.19  |\n",
      "|116.04 |\n",
      "|153.33 |\n",
      "|2333.32|\n",
      "|1873.33|\n",
      "|999.7  |\n",
      "|70.52  |\n",
      "|73.02  |\n",
      "|122.78 |\n",
      "|532.46 |\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Unique values for column: avg_rainfall_mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|avg_rainfall_mm|\n",
      "+---------------+\n",
      "|284.42         |\n",
      "|259.33         |\n",
      "|723.67         |\n",
      "|321.58         |\n",
      "|249.0          |\n",
      "|296.45         |\n",
      "|209.56         |\n",
      "|677.82         |\n",
      "|371.55         |\n",
      "|481.28         |\n",
      "|261.79         |\n",
      "|314.38         |\n",
      "|310.56         |\n",
      "|262.84         |\n",
      "|494.02         |\n",
      "|169.64         |\n",
      "|254.62         |\n",
      "|162.83         |\n",
      "|134.11         |\n",
      "|461.47         |\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in food_price_data.columns:\n",
    "    print(f\"\\nUnique values for column: {column}\")\n",
    "    food_price_data.select(column).distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "262fce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+--------------------+\n",
      "|price |unit |normalized_price|log_normalized_price|\n",
      "+------+-----+----------------+--------------------+\n",
      "|38.44 |KG   |38.44           |3.6747805297344347  |\n",
      "|79.99 |KG   |79.99           |4.3943256902608985  |\n",
      "|5738.0|90 KG|63.76           |4.170688128809434   |\n",
      "|74.0  |KG   |74.0            |4.31748811353631    |\n",
      "|53.36 |KG   |53.36           |3.995628589282943   |\n",
      "+------+-----+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, round as spark_round, log1p\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# List of units to drop\n",
    "units_to_drop = [\"Unit\", \"Bunch\", \"Head\"]\n",
    "food_price_data = food_price_data.filter(~col(\"unit\").isin(units_to_drop))\n",
    "\n",
    "# Define conversion factors\n",
    "unit_conversion = {\n",
    "    '400 G': 2.5,\n",
    "    '64 KG': 1/64,\n",
    "    'L': 1,\n",
    "    '200 G': 5,\n",
    "    '50 KG': 1/50,\n",
    "    '13 KG': 1/13,\n",
    "    '90 KG': 1/90,\n",
    "    '200 ML': 5,\n",
    "    '126 KG': 1/126,\n",
    "    'KG': 1,\n",
    "    '500 ML': 2\n",
    "}\n",
    "\n",
    "# Create UDF\n",
    "def normalize_price(price, unit):\n",
    "    factor = unit_conversion.get(unit, 1)\n",
    "    return price * factor\n",
    "\n",
    "normalize_price_udf = udf(normalize_price, DoubleType())\n",
    "\n",
    "# Apply normalization\n",
    "food_price_data = food_price_data.withColumn(\"normalized_price\", normalize_price_udf(col(\"price\"), col(\"unit\")))\n",
    "\n",
    "# Round to 2 decimal places\n",
    "food_price_data = food_price_data.withColumn(\"normalized_price\", spark_round(col(\"normalized_price\"), 2))\n",
    "\n",
    "# log transform to reduce skewness\n",
    "food_price_data = food_price_data.withColumn(\"log_normalized_price\", log1p(col(\"normalized_price\")))\n",
    "\n",
    "# Show first 5 rows\n",
    "food_price_data.select(\"price\", \"unit\", \"normalized_price\", \"log_normalized_price\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a666b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+-------+-------+------------------+-----------+-----+---------+------+---------------+----------------+--------------------+\n",
      "|year|month| region| county| market|          category|  commodity| unit|pricetype| price|avg_rainfall_mm|normalized_price|log_normalized_price|\n",
      "+----+-----+-------+-------+-------+------------------+-----------+-----+---------+------+---------------+----------------+--------------------+\n",
      "|2014|    1|  Coast|Mombasa|Mombasa|cereals and tubers|      Maize|   KG|Wholesale| 38.44|         259.33|           38.44|  3.6747805297344347|\n",
      "|2014|    1|  Coast|Mombasa|Mombasa|   pulses and nuts|      Beans|   KG|Wholesale| 79.99|         259.33|           79.99|  4.3943256902608985|\n",
      "|2014|    1|  Coast|Mombasa|Mombasa|   pulses and nuts|Beans (dry)|90 KG|Wholesale|5738.0|         259.33|           63.76|   4.170688128809434|\n",
      "|2014|    1|Eastern|  Kitui|  Kitui|   pulses and nuts|Beans (dry)|   KG|   Retail|  74.0|         259.33|            74.0|    4.31748811353631|\n",
      "+----+-----+-------+-------+-------+------------------+-----------+-----+---------+------+---------------+----------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "food_price_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d2b6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/FoodPriceData\"\n",
    "\n",
    "food_price_data.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2ae2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553406b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404e8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99dcdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba70d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "pdf = food_price_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49369066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# County & Region\n",
    "county_region_df = pdf[['county', 'market', 'region']].drop_duplicates().sort_values(by='county')\n",
    "county_region_df.to_csv('data/county_region.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8cf3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commodity, Category, Normalized Price\n",
    "commodity_cat_price_df = pdf[['year', 'month','commodity', 'category', 'normalized_price']].drop_duplicates().sort_values(by='commodity')\n",
    "commodity_cat_price_df.to_csv('data/commodity_category_price.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e267da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year, Month, Avg Rainfall\n",
    "rainfall_df = pdf[['year', 'month', 'avg_rainfall_mm']].drop_duplicates().sort_values(by=['year', 'month'])\n",
    "rainfall_df.to_csv('data/monthly_rainfall.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
