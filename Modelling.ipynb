{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1hplIk6mZ7-OhWV5bLJTr3a2OFFYOt4JP","authorship_tag":"ABX9TyPfrVXWsAG623A3/3wGCKt3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vhg-UImr7Cqc","executionInfo":{"status":"ok","timestamp":1741027132164,"user_tz":-180,"elapsed":2025,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"a9eb18be-03cf-452f-e8cd-bf78cb61a552"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.evaluation import RegressionEvaluator\n","import torch\n","import torch.nn as nn\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"Ire1GwAF7sfp","executionInfo":{"status":"ok","timestamp":1741027135579,"user_tz":-180,"elapsed":16,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"Food Price Prediction\").getOrCreate()"],"metadata":{"id":"jB0bFo9I7zYN","executionInfo":{"status":"ok","timestamp":1741027140008,"user_tz":-180,"elapsed":30,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"IUpWsTY7BDFh"}},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Distributed-Food-Price-Prediction-for-Kenyan-Markets/data/cleaned_data.csv'\n","data = spark.read.csv(path, header=True, inferSchema=True)"],"metadata":{"id":"jCjU2Mcv8BS_","executionInfo":{"status":"ok","timestamp":1741027145588,"user_tz":-180,"elapsed":727,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["data.show(30)"],"metadata":{"id":"GmRmRb4j9rSP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741027149293,"user_tz":-180,"elapsed":238,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"d0873638-4fe0-4c6a-c83e-449d9b9e6700"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------+-----------+--------------------+---------+---------+--------------------+----------+----------------+-------+----+-----+--------------+---------------+--------+-------------------+----------+---------------------+------------+------------------+------------------+\n","|     province|   district|              market| latitude|longitude|           commodity|      unit|       priceflag|  price|year|month|milk and dairy|pulses and nuts|non-food|meat, fish and eggs|#item+type|vegetables and fruits|oil and fats|cereals and tubers|miscellaneous food|\n","+-------------+-----------+--------------------+---------+---------+--------------------+----------+----------------+-------+----+-----+--------------+---------------+--------+-------------------+----------+---------------------+------------+------------------+------------------+\n","|   #adm1+name| #adm2+name|    #loc+market+name| #geo+lat| #geo+lon|          #item+name|#item+unit|#item+price+flag| #value|NULL| NULL|             0|              0|       0|                  0|         1|                    0|           0|                 0|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|               Maize|        KG|          actual|  16.13|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|       Maize (white)|     90 KG|          actual| 1480.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|               Beans|        KG|          actual|  33.63|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|         Beans (dry)|     90 KG|          actual| 3246.0|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|      Eastern|      Kitui|               Kitui|-1.366667|38.016667|       Maize (white)|        KG|          actual|   17.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Eastern|      Kitui|               Kitui|-1.366667|38.016667|    Potatoes (Irish)|     50 KG|          actual|1249.99|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Eastern|      Kitui|               Kitui|-1.366667|38.016667|             Sorghum|     90 KG|          actual| 1800.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Eastern|      Kitui|               Kitui|-1.366667|38.016667|         Beans (dry)|        KG|          actual|   39.0|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|      Eastern|   Marsabit|            Marsabit| 2.333333|37.983333|       Maize (white)|        KG|          actual|   21.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|               Bread|     400 G|          actual|   26.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|               Maize|        KG|          actual|  15.48|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|       Maize (white)|     90 KG|          actual| 1399.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|    Potatoes (Irish)|     50 KG|          actual| 664.43|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|             Sorghum|     90 KG|          actual| 1960.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|Milk (cow, pasteu...|    500 ML|          actual|   22.0|2006|    1|             1|              0|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|     Oil (vegetable)|         L|          actual|  115.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           1|                 0|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|               Beans|        KG|          actual|  42.31|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|      Nairobi|    Nairobi|             Nairobi|-1.283333|36.816667|         Beans (dry)|     90 KG|          actual| 3175.0|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|North Eastern|    Mandera|             Mandera| 3.936804|41.858383|       Maize (white)|        KG|          actual|   30.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|       Nyanza|     Kisumu|              Kisumu|     -0.1|    34.75|               Maize|        KG|          actual|  14.84|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|       Nyanza|     Kisumu|              Kisumu|     -0.1|    34.75|       Maize (white)|     90 KG|          actual| 1320.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|       Nyanza|     Kisumu|              Kisumu|     -0.1|    34.75|         Beans (dry)|     90 KG|          actual| 4000.0|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|  Rift Valley|    Turkana|    Lodwar (Turkana)| 3.116667|     35.6|       Maize (white)|        KG|          actual|   26.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|  Rift Valley|Uasin Gishu|Eldoret town (Uas...| 0.516667|35.283333|               Maize|        KG|          actual|  12.96|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|  Rift Valley|Uasin Gishu|Eldoret town (Uas...| 0.516667|35.283333|       Maize (white)|     90 KG|          actual| 1192.0|2006|    1|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|  Rift Valley|Uasin Gishu|Eldoret town (Uas...| 0.516667|35.283333|               Beans|        KG|          actual|  45.85|2006|    1|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|               Maize|        KG|          actual|  16.41|2006|    2|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|       Maize (white)|     90 KG|          actual| 1523.0|2006|    2|             0|              0|       0|                  0|         0|                    0|           0|                 1|                 0|\n","|        Coast|    Mombasa|             Mombasa|    -4.05|39.666667|         Beans (dry)|     90 KG|          actual| 3400.0|2006|    2|             0|              1|       0|                  0|         0|                    0|           0|                 0|                 0|\n","+-------------+-----------+--------------------+---------+---------+--------------------+----------+----------------+-------+----+-----+--------------+---------------+--------+-------------------+----------+---------------------+------------+------------------+------------------+\n","only showing top 30 rows\n","\n"]}]},{"cell_type":"code","source":["data.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xej0wFuMU49A","executionInfo":{"status":"ok","timestamp":1741027153324,"user_tz":-180,"elapsed":15,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"660f79e7-a325-49c5-9528-5aac9a308885"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- province: string (nullable = true)\n"," |-- district: string (nullable = true)\n"," |-- market: string (nullable = true)\n"," |-- latitude: string (nullable = true)\n"," |-- longitude: string (nullable = true)\n"," |-- commodity: string (nullable = true)\n"," |-- unit: string (nullable = true)\n"," |-- priceflag: string (nullable = true)\n"," |-- price: string (nullable = true)\n"," |-- year: integer (nullable = true)\n"," |-- month: integer (nullable = true)\n"," |-- milk and dairy: integer (nullable = true)\n"," |-- pulses and nuts: integer (nullable = true)\n"," |-- non-food: integer (nullable = true)\n"," |-- meat, fish and eggs: integer (nullable = true)\n"," |-- #item+type: integer (nullable = true)\n"," |-- vegetables and fruits: integer (nullable = true)\n"," |-- oil and fats: integer (nullable = true)\n"," |-- cereals and tubers: integer (nullable = true)\n"," |-- miscellaneous food: integer (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["# Feature selection\n","selected_features = ['year', 'month', 'commodity', 'longitude', 'latitude']\n","label = 'price'"],"metadata":{"id":"MD4hE88CU6zK","executionInfo":{"status":"ok","timestamp":1741027168493,"user_tz":-180,"elapsed":10,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# String indexing for categorical features\n","indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(data) for col in ['commodity']]\n","for indexer in indexers:\n","    data = indexer.transform(data)"],"metadata":{"id":"7VkrbNwTDfwf","executionInfo":{"status":"ok","timestamp":1741027175657,"user_tz":-180,"elapsed":575,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Converting 'latitude', 'longitude', and 'price' to double\n","data = data.withColumn('latitude', col('latitude').cast('double'))\n","data = data.withColumn('longitude', col('longitude').cast('double'))\n","data = data.withColumn('price', col('price').cast('double'))\n","\n","# Assembling features again\n","from pyspark.ml.feature import VectorAssembler\n","assembler = VectorAssembler(inputCols=['year', 'month', 'commodity_index', 'longitude', 'latitude'], outputCol='features')\n","\n","# Transform data\n","data = assembler.transform(data)"],"metadata":{"id":"EwaQoglkzhUL","executionInfo":{"status":"ok","timestamp":1741027202880,"user_tz":-180,"elapsed":101,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Train-test split\n","train_data, test_data = data.randomSplit([0.8, 0.2])"],"metadata":{"id":"S2g6jNKhPafH","executionInfo":{"status":"ok","timestamp":1741027208642,"user_tz":-180,"elapsed":40,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3IGsgq1hPaTB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PySpark MLlib: Linear Regression"],"metadata":{"id":"rxWzUCkTP63T"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Cast 'price' column to float\n","train_data = train_data.withColumn('price', col('price').cast('float'))\n","test_data = test_data.withColumn('price', col('price').cast('float'))\n","\n","# Fit the Linear Regression model\n","from pyspark.ml.regression import LinearRegression\n","lr = LinearRegression(featuresCol='features', labelCol='price')\n","\n","lr_model = lr.fit(train_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yXJQSgICPaIx","executionInfo":{"status":"error","timestamp":1741026785529,"user_tz":-180,"elapsed":2261,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"b149880e-b1c2-4dc8-a7d6-728f24fecc6a"},"execution_count":27,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o320.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 10) (f930c2b15197 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3763/0x00000008415b5840`: (struct<year_double_VectorAssembler_1ec685bebb4b:double,month_double_VectorAssembler_1ec685bebb4b:double,commodity_index:double,longitude_double_VectorAssembler_1ec685bebb4b:double,latitude_double_VectorAssembler_1ec685bebb4b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3763/0x00000008415b5840`: (struct<year_double_VectorAssembler_1ec685bebb4b:double,month_double_VectorAssembler_1ec685bebb4b:double,commodity_index:double,longitude_double_VectorAssembler_1ec685bebb4b:double,latitude_double_VectorAssembler_1ec685bebb4b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-5b009ee57387>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o320.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 10) (f930c2b15197 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3763/0x00000008415b5840`: (struct<year_double_VectorAssembler_1ec685bebb4b:double,month_double_VectorAssembler_1ec685bebb4b:double,commodity_index:double,longitude_double_VectorAssembler_1ec685bebb4b:double,latitude_double_VectorAssembler_1ec685bebb4b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3763/0x00000008415b5840`: (struct<year_double_VectorAssembler_1ec685bebb4b:double,month_double_VectorAssembler_1ec685bebb4b:double,commodity_index:double,longitude_double_VectorAssembler_1ec685bebb4b:double,latitude_double_VectorAssembler_1ec685bebb4b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"P2Zs6X3OPZ8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"3UoMQrOPIJ1H"}},{"cell_type":"code","source":["rf_model.save('/content/drive/MyDrive/Distributed-Food-Price-Prediction-for-Kenyan-Markets/food_price_model')"],"metadata":{"id":"BdkgHAh2IMNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XhvI3Sn_KV2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8-ECTMxCKVv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V_hmGLq4KVuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hsJmQk3uKViV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Git Version control"],"metadata":{"id":"p9NLmqNpKX8g"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Distributed-Food-Price-Prediction-for-Kenyan-Markets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvgAwUSjKl0D","executionInfo":{"status":"ok","timestamp":1740286629750,"user_tz":-180,"elapsed":12,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"ee1d24bc-e310-481c-c3a8-bd358570baaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Distributed-Food-Price-Prediction-for-Kenyan-Markets\n"]}]},{"cell_type":"code","source":["!git pull origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APG3e0cWKVdR","executionInfo":{"status":"ok","timestamp":1740286639081,"user_tz":-180,"elapsed":5444,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"bd2dcfa2-16d9-4366-e402-046e96514145"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["From https://github.com/brc0d3s/Distributed-Food-Price-Prediction-for-Kenyan-Markets\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"brc0d3s@gmail.com\"\n","!git config --global user.name \"brc0d3s\""],"metadata":{"id":"G6_XJtN5KVbV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add ."],"metadata":{"id":"j0DaqvYKKVZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"model\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHRaF-p1KVXT","executionInfo":{"status":"ok","timestamp":1740286761049,"user_tz":-180,"elapsed":2310,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"c8b7e83e-afc6-4592-d836-7bf5d4c7f9eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[dev 2d182a7] model\n"," 16 files changed, 3 insertions(+), 504 deletions(-)\n"," rewrite Modelling.ipynb (97%)\n"," create mode 100644 food_price_model/data/._SUCCESS.crc\n"," create mode 100644 food_price_model/data/.part-00000-f990ddec-df7f-4509-a3f4-03829edad1cc-c000.snappy.parquet.crc\n"," create mode 100644 food_price_model/data/_SUCCESS\n"," create mode 100644 food_price_model/data/part-00000-f990ddec-df7f-4509-a3f4-03829edad1cc-c000.snappy.parquet\n"," create mode 100644 food_price_model/metadata/._SUCCESS.crc\n"," create mode 100644 food_price_model/metadata/.part-00000.crc\n"," create mode 100644 food_price_model/metadata/_SUCCESS\n"," create mode 100644 food_price_model/metadata/part-00000\n"," create mode 100644 food_price_model/treesMetadata/._SUCCESS.crc\n"," create mode 100644 food_price_model/treesMetadata/.part-00000-f9498d39-f685-4c1d-9604-32c46b66f1e5-c000.snappy.parquet.crc\n"," create mode 100644 food_price_model/treesMetadata/_SUCCESS\n"," create mode 100644 food_price_model/treesMetadata/part-00000-f9498d39-f685-4c1d-9604-32c46b66f1e5-c000.snappy.parquet\n"," delete mode 100644 raw_data/crops_data_production.csv\n"]}]},{"cell_type":"code","source":["!git push origin dev"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ns7jWUYtKVJU","executionInfo":{"status":"ok","timestamp":1740286779077,"user_tz":-180,"elapsed":2394,"user":{"displayName":"BRIAN OMONDI","userId":"10412797874550437401"}},"outputId":"f4350e97-d106-4c36-b2fa-9a7139304a14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 23, done.\n","Counting objects: 100% (23/23), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (15/15), done.\n","Writing objects: 100% (18/18), 2.94 MiB | 5.08 MiB/s, done.\n","Total 18 (delta 4), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n","To https://github.com/brc0d3s/Distributed-Food-Price-Prediction-for-Kenyan-Markets.git\n","   44b1be2..2d182a7  dev -> dev\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_IdSl2wbKVE5"},"execution_count":null,"outputs":[]}]}